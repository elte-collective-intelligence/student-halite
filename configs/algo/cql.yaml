# Centralized Q-Learning algorithm configuration
_target_: src.algo.cql.CentralizedQLearning

lr: 0.0001               # Learning rate for Q-network optimization
gamma: 0.99              # Discount factor for future rewards (higher values prioritize long-term rewards)
epsilon_start: 1.0       # Initial exploration rate for epsilon-greedy policy
epsilon_end: 0.05        # Final exploration rate after decay
epsilon_decay: 0.99995   # Decay factor applied per update to reduce exploration over time
buffer_size: 50000       # Maximum number of experience tuples stored in replay buffer
batch_size: 16           # Number of experience samples used per training update
target_update_freq: 50   # Frequency (in updates) for updating target Q-network weights


